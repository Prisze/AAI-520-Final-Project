{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "06a6e869",
   "metadata": {},
   "source": [
    "## Chatbot Design using the Ubuntu Dialogue Corpus Dataset\n",
    "## Group 6: Priscilla Marquez, Johnathan Long, Greg Moore\n",
    "## Applied Artificial Intelligence (AAI), University of San Diego\n",
    "## AAI-520: Natural Language Processing and GenAI\n",
    "## Professor Kahila Mokhtari, PhD\n",
    "## October 21, 2024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ce99f287",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Display Ubuntu Dataset:\n",
      "          folder dialogueID                      date        from       to  \\\n",
      "0           301      1.tsv  2004-11-23T11:49:00.000Z     stuNNed      NaN   \n",
      "1           301      1.tsv  2004-11-23T11:49:00.000Z     crimsun  stuNNed   \n",
      "2           301      1.tsv  2004-11-23T11:49:00.000Z     stuNNed  crimsun   \n",
      "3           301      1.tsv  2004-11-23T11:49:00.000Z     crimsun  stuNNed   \n",
      "4           301      1.tsv  2004-11-23T11:50:00.000Z     stuNNed  crimsun   \n",
      "...         ...        ...                       ...         ...      ...   \n",
      "9212872      13   3676.tsv  2012-07-07T20:17:00.000Z  MonkeyDust  legolas   \n",
      "9212873      13   3676.tsv  2012-07-07T20:18:00.000Z  MonkeyDust  legolas   \n",
      "9212874      13  16586.tsv  2008-07-25T01:53:00.000Z    linuxfce      NaN   \n",
      "9212875      13  16586.tsv  2008-07-25T01:53:00.000Z    linuxfce      NaN   \n",
      "9212876      13  16586.tsv  2008-07-25T01:54:00.000Z    linuxfce      NaN   \n",
      "\n",
      "                                                      text  \n",
      "0         any ideas why java plugin takes so long to load?  \n",
      "1                                                java 1.4?  \n",
      "2                                                      yes  \n",
      "3                             java 1.5 loads _much_ faster  \n",
      "4        noneus: how can i get 1.5 is there a .deb some...  \n",
      "...                                                    ...  \n",
      "9212872                                            = arian  \n",
      "9212873             observation and deduction, dear watson  \n",
      "9212874  i am trying to install nvidia drivers from the...  \n",
      "9212875  how do i enter runlevel 3? when i try telinit ...  \n",
      "9212876     anyone know how to enter runlevel 3 in ubuntu?  \n",
      "\n",
      "[9212877 rows x 6 columns]\n"
     ]
    }
   ],
   "source": [
    "####################################################################\n",
    "\n",
    "# Project Overview:\n",
    "#\n",
    "# Goal: Build a chatbot that can carry out multi-turn conversations, adapt\n",
    "# to context, and handle a variety of topics.\n",
    "# Output: A web or app interface where users can converse with the chatbot.\n",
    "#\n",
    "# Pre-requisites:\n",
    "# Basic understanding of deep learning and neural networks.\n",
    "# Familiarity with a deep learning framework (e.g., TensorFlow, PyTorch).\n",
    "# Basic knowledge of web development (for the interface).\n",
    "#\n",
    "# Phases:\n",
    "#   \n",
    "# Research and Study Phase:\n",
    "# Study generative-based chatbot architectures like Seq2Seq, Transformers,\n",
    "# and GPT and deep learning. Understand the challenges of chatbot design:\n",
    "# context management, coherency, handling ambiguous queries, etc.\n",
    "#\n",
    "# Data Collection and Preprocessing:\n",
    "# We chose to use the Ubuntu Dialogue Corpus Dataset\n",
    "# https://www.kaggle.com/datasets/rtatman/ubuntu-dialogue-corpus\n",
    "#\n",
    "# Model Design and Training:\n",
    "# Choose an architecture (e.g., Transformer-based models or deep learning models).\n",
    "# Implement or leverage existing implementations to train the model with the dataset.\n",
    "#\n",
    "# Evaluation:\n",
    "# Implement evaluation metrics.\n",
    "\n",
    "##################################################\n",
    "\n",
    "# Load the libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer, AdamW, get_linear_schedule_with_warmup\n",
    "# stop the future warning\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# We will use the text column in the Ubuntu Dialogue Corpus\n",
    "# Read in entire Ubuntu dialogueText_196.csv dataset\n",
    "ubuntu = pd.read_csv('C:/Users/gregm/.spyder-py3/AAI_520/FINAL/dialogueText_196.csv')\n",
    "# Output Ubuntu Dataset\n",
    "print('\\nDisplay Ubuntu Dataset:\\n', ubuntu)\n",
    "\n",
    "##################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f2076451",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Head of First 5 texts:\n",
      "    folder dialogueID                      date     from       to  \\\n",
      "0     301      1.tsv  2004-11-23T11:49:00.000Z  stuNNed      NaN   \n",
      "1     301      1.tsv  2004-11-23T11:49:00.000Z  crimsun  stuNNed   \n",
      "2     301      1.tsv  2004-11-23T11:49:00.000Z  stuNNed  crimsun   \n",
      "3     301      1.tsv  2004-11-23T11:49:00.000Z  crimsun  stuNNed   \n",
      "4     301      1.tsv  2004-11-23T11:50:00.000Z  stuNNed  crimsun   \n",
      "\n",
      "                                                text  \n",
      "0   any ideas why java plugin takes so long to load?  \n",
      "1                                          java 1.4?  \n",
      "2                                                yes  \n",
      "3                       java 1.5 loads _much_ faster  \n",
      "4  noneus: how can i get 1.5 is there a .deb some...  \n"
     ]
    }
   ],
   "source": [
    "##################################################\n",
    "\n",
    "# Text Preprocessing\n",
    "# Extract subset of Ubuntu\n",
    "text = ubuntu.iloc[:500000]\n",
    "# Extract subset of 100 conversations for test and evaluation\n",
    "eval_dataset_df = ubuntu.iloc[500000:500100]\n",
    "\n",
    "# Output the first 5 texts\n",
    "print('\\nHead of First 5 texts:\\n', text.head(5))\n",
    "\n",
    "##################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a015dd8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "         folder dialogueID                      date      from        to  \\\n",
      "0          301      1.tsv  2004-11-23T11:49:00.000Z   stuNNed       NaN   \n",
      "1          301      1.tsv  2004-11-23T11:49:00.000Z   crimsun   stuNNed   \n",
      "2          301      1.tsv  2004-11-23T11:49:00.000Z   stuNNed   crimsun   \n",
      "3          301      1.tsv  2004-11-23T11:49:00.000Z   crimsun   stuNNed   \n",
      "4          301      1.tsv  2004-11-23T11:50:00.000Z   stuNNed   crimsun   \n",
      "...        ...        ...                       ...       ...       ...   \n",
      "499995      25   4002.tsv  2007-07-28T11:13:00.000Z    reanjr  xtknight   \n",
      "499996      25   4002.tsv  2007-07-28T11:23:00.000Z    reanjr  xtknight   \n",
      "499997      25   4002.tsv  2007-07-28T11:23:00.000Z  xtknight    reanjr   \n",
      "499998      25   4002.tsv  2007-07-28T11:26:00.000Z    reanjr  xtknight   \n",
      "499999      25   4002.tsv  2007-07-28T12:10:00.000Z    reanjr  xtknight   \n",
      "\n",
      "                                                     text  \n",
      "0        any ideas why java plugin takes so long to load?  \n",
      "1                                               java 1.4?  \n",
      "2                                                     yes  \n",
      "3                            java 1.5 loads _much_ faster  \n",
      "4       noneus: how can i get 1.5 is there a .deb some...  \n",
      "...                                                   ...  \n",
      "499995                              lemme think for a sec  \n",
      "499996         just an update, I think I've almost got it  \n",
      "499997                                               cool  \n",
      "499998  paginated search could be done as well, though...  \n",
      "499999                                   I totally concur  \n",
      "\n",
      "[500000 rows x 6 columns]\n"
     ]
    }
   ],
   "source": [
    "##################################################\n",
    "\n",
    "# Data Preparation and Pre-processing\n",
    "class DialogueDataset(Dataset):\n",
    "    def __init__(self, dialogues, tokenizer, max_length=512):\n",
    "        self.dialogues = dialogues\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dialogues)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        dialogue = self.dialogues[idx]\n",
    "        encoded = self.tokenizer.encode_plus(\n",
    "            dialogue,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_length,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        return {\n",
    "            'input_ids': encoded['input_ids'].squeeze(),\n",
    "            'attention_mask': encoded['attention_mask'].squeeze()\n",
    "        }\n",
    "\n",
    "\n",
    "# Pre-process data\n",
    "dialogues = text\n",
    "# Set tokenizer\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('microsoft/DialoGPT-medium')\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "# Pre-process text data\n",
    "dataset = DialogueDataset(dialogues, tokenizer)\n",
    "dataloader = DataLoader(dataset, batch_size=16, shuffle=True)\n",
    "print('\\n',dialogues)\n",
    "\n",
    "##################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3f09e30",
   "metadata": {},
   "source": [
    "## From Priscilla: added last line to process eval_conversations for eval metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4ceb21a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################################\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "def preprocess_conversations(df):\n",
    "    # Remove rows with any NaN values\n",
    "    df_clean = df.dropna(subset=['text'])\n",
    "\n",
    "    # Filter dialogues with exactly 2 participants\n",
    "    valid_dialogues = df_clean.groupby('dialogueID').filter(lambda x: x['from'].nunique() == 2)\n",
    "\n",
    "    # Sort the dataframe by 'dialogueID' and 'date' to ensure conversation order\n",
    "    valid_dialogues = valid_dialogues.sort_values(by=['dialogueID', 'date'])\n",
    "\n",
    "    # Merge consecutive messages from the same sender in each dialogue\n",
    "    merged_dialogues = []\n",
    "\n",
    "    for dialogue_id, group in valid_dialogues.groupby('dialogueID'):\n",
    "        group = group.reset_index(drop=True)  # Reset index to avoid issues when iterating\n",
    "\n",
    "        # Initialize a list to collect messages for the current conversation\n",
    "        conversation = []\n",
    "\n",
    "        # Loop through the messages in this dialogue\n",
    "        current_speaker = group.loc[0, 'from']\n",
    "        current_message = group.loc[0, 'text']\n",
    "\n",
    "        for i in range(1, len(group)):\n",
    "            if group.loc[i, 'from'] == current_speaker:\n",
    "                # If the next message is from the same speaker, merge it\n",
    "                current_message += \" \" + group.loc[i, 'text']\n",
    "            else:\n",
    "                # If the next message is from a different speaker, append the current speaker's message\n",
    "                conversation.append(current_message)\n",
    "                # Switch to the new speaker\n",
    "                current_speaker = group.loc[i, 'from']\n",
    "                current_message = group.loc[i, 'text']\n",
    "\n",
    "        # Append the last message\n",
    "        conversation.append(current_message)\n",
    "\n",
    "        # Concatenate messages in the dialogue using the '<|endoftext|>' separator\n",
    "        merged_dialogue = '<|endoftext|>'.join(conversation)\n",
    "        merged_dialogues.append(merged_dialogue)\n",
    "\n",
    "    return merged_dialogues\n",
    "\n",
    "# Preprocess the evaluation metrics conversations\n",
    "eval_conversations = preprocess_conversations(eval_dataset_df)\n",
    "\n",
    "##################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48c8fc90",
   "metadata": {},
   "source": [
    "## Calculate Perplexity Score for ChatBot Model using Ubuntu Eval Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f2a01ece",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChatBot Model Perplexity Score: 199.4624786376953\n"
     ]
    }
   ],
   "source": [
    "##################################################\n",
    "\n",
    "# Calculate Perplexity Score for ChatBot Model using Ubuntu Eval Data\n",
    "# Implement a sliding window approach to calculate perplexity for a LLM\n",
    "# ChatBot Model on long sequences that exceed the model's max context length\n",
    "# Import GPT2 model with language modeling head and tokenizer and PyTorch\n",
    "from transformers import GPT2LMHeadModel, GPT2TokenizerFast, GPT2Tokenizer \n",
    "import torch\n",
    "\n",
    "# Define a function to calculate the perplexity of conversation text\n",
    "def calc_perplexity_score(model, tokenizer, text):\n",
    "    # Tokenize the input text. Convert the text to token IDs and pad or truncate \n",
    "    # to max length of 1000 tokens, return result as PyTorch tensors\n",
    "    encodings = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True, max_length=1000)\n",
    "    # Max sequence length set to 1000\n",
    "    max_length = 1000\n",
    "    # Set the step size for the sliding window approach used in perplexity\n",
    "    stride = 512\n",
    "    # Initialize list for negative log-likelihoods\n",
    "    nlls = []\n",
    "    # Loop through input sequence of step size stride\n",
    "    for i in range(0, encodings.input_ids.size(1), stride):\n",
    "        # Calculate start and end positions for window\n",
    "        strt_pos = max(i + stride - max_length, 0)\n",
    "        end_pos = min(i + stride, encodings.input_ids.size(1))\n",
    "        # Calculate target length\n",
    "        trg_len = end_pos - i\n",
    "        # Extract input IDs for window, make a copy for targets\n",
    "        input_ids = encodings.input_ids[:, strt_pos:end_pos].to(device)\n",
    "        target_ids = input_ids.clone()\n",
    "        # Set all but last tokens to -100 (ignore in loss computation)\n",
    "        target_ids[:, :-trg_len] = -100\n",
    "        # Run model on input, compute loss, calculate negative log-likelihood\n",
    "        # Prevent gradient computation to save memory\n",
    "        with torch.no_grad():\n",
    "            outputs = model(input_ids, labels=target_ids)\n",
    "            neg_log_like = outputs.loss * trg_len\n",
    "        # Add computed negative log-likelihood to the list\n",
    "        nlls.append(neg_log_like)\n",
    "    # Compute perplexity by summing negative log-likelihoods, divide by total\n",
    "    # length, take the exponential and return perplexity score as a number\n",
    "    ppl = torch.exp(torch.stack(nlls).sum() / end_pos)\n",
    "    return ppl.item()\n",
    "\n",
    "# Set tokenizer\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('microsoft/DialoGPT-medium')\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "# Read in ChatBot Model\n",
    "model = GPT2LMHeadModel.from_pretrained('C:/Users/gregm/.spyder-py3/AAI_520/FINAL')\n",
    "# Use evaluation conversations to calculate perplexity score\n",
    "ubuntu_text = eval_conversations\n",
    "perplexity = calc_perplexity_score(model, tokenizer, ubuntu_text)\n",
    "print(f\"ChatBot Model Perplexity Score: {perplexity}\")\n",
    "\n",
    "##################################################"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
