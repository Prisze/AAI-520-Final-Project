{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "import numpy as np\n",
    "import torch\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "\n",
    "# Set device to CUDA if available, else CPU\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Load the pre-trained model and tokenizer\n",
    "model_path = \"C:\\\\Users\\\\thema\\\\Desktop\\\\Content_2\\\\USD\\\\Natural Language Processing\\\\Final Project\\\\Model\"\n",
    "model = GPT2LMHeadModel.from_pretrained(model_path).to(device)\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('microsoft/DialoGPT-medium')\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "\n",
    "class Chat:\n",
    "    def __init__(self, chatbot, max_response_len=20):\n",
    "        self.chatbot = chatbot\n",
    "        self.messages = []\n",
    "        self.max_response_len = max_response_len\n",
    "        self.temperature = 0.2  # Default temperature\n",
    "        self.top_p = 0.3  # Default top_p\n",
    "\n",
    "    def send(self, text: str):\n",
    "        response = self.handle_input(text)\n",
    "        self.messages.append(f\"User: {text}\")\n",
    "        self.messages.append(f\"Bot: {response}\")\n",
    "        return response\n",
    "\n",
    "    def set_topic(self, topic: str):\n",
    "        self.messages.append(f\"Topic: {topic}\")\n",
    "\n",
    "    def reset_conversation(self):\n",
    "        self.messages = []\n",
    "\n",
    "    def set_mood(self, temperature: float, top_p: float):\n",
    "        self.temperature = temperature\n",
    "        self.top_p = top_p\n",
    "\n",
    "    def handle_input(self, text: str):\n",
    "        input_type = self.classify_input(text)\n",
    "        if input_type == \"question\":\n",
    "            response = self.chatbot.generate(self.get_context() + f\"User: {text}\", max_length=self.max_response_len, temperature=self.temperature, top_p=self.top_p)\n",
    "        elif input_type == \"acknowledgment\":\n",
    "            response = self.generate_acknowledgment_response(text)\n",
    "        else:\n",
    "            response = self.generate_follow_up(text)\n",
    "        return response\n",
    "\n",
    "    def classify_input(self, text: str) -> str:\n",
    "        if \"?\" in text or text.lower().startswith((\"how\", \"what\", \"why\", \"where\", \"when\", \"who\")):\n",
    "            return \"question\"\n",
    "        elif text.lower() in [\"thanks\", \"thank you\", \"okay\", \"got it\", \"appreciate it\"]:\n",
    "            return \"acknowledgment\"\n",
    "        else:\n",
    "            return \"statement\"\n",
    "\n",
    "    def generate_acknowledgment_response(self, text: str) -> str:\n",
    "        prompt = self.get_context() + f\"User: {text}\\n Chatbot: You're welcome! I hope that helps.\"\n",
    "        response = self.chatbot.generate(prompt, max_length=self.max_response_len, temperature=self.temperature, top_p=self.top_p)\n",
    "        return response\n",
    "\n",
    "    def generate_follow_up(self, text: str) -> str:\n",
    "        prompt = self.get_context() + f\"User: {text}\\n Chatbot: That's an interesting point.\"\n",
    "        response = self.chatbot.generate(prompt, max_length=self.max_response_len, temperature=self.temperature, top_p=self.top_p)\n",
    "        return response\n",
    "\n",
    "    def get_context(self) -> str:\n",
    "        prompt = '<|endoftext|>'.join(self.messages) + '<|endoftext|>'\n",
    "        max_tokens = 112\n",
    "        prompt_tokens = self.chatbot.tokenizer.encode(prompt)\n",
    "        if len(prompt_tokens) > max_tokens:\n",
    "            prompt_tokens = prompt_tokens[-max_tokens:]\n",
    "            prompt = self.chatbot.tokenizer.decode(prompt_tokens)\n",
    "        return prompt\n",
    "\n",
    "\n",
    "class ChatBot:\n",
    "    def __init__(self, model_path: str, device=None):\n",
    "        if not device:\n",
    "            device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.device = device\n",
    "        self.model = GPT2LMHeadModel.from_pretrained(model_path).to(self.device)\n",
    "        self.tokenizer = GPT2Tokenizer.from_pretrained('microsoft/DialoGPT-medium')\n",
    "        self.tokenizer.pad_token = self.tokenizer.eos_token\n",
    "\n",
    "    def generate(self, text: str, max_length: int = 12, temperature: float = 0.6, top_p: float = 0.9) -> str:\n",
    "        with torch.no_grad():\n",
    "            inputs = self.tokenizer(text, return_tensors=\"pt\")\n",
    "            outputs = self.model.generate(\n",
    "                inputs.input_ids.to(self.device),\n",
    "                attention_mask=inputs.attention_mask.to(self.device),\n",
    "                max_new_tokens=50,\n",
    "                no_repeat_ngram_size=5,\n",
    "                early_stopping=True,\n",
    "                num_beams=2,\n",
    "                pad_token_id=self.tokenizer.eos_token_id,\n",
    "                eos_token_id=self.tokenizer.eos_token_id,\n",
    "                repetition_penalty=10.0,\n",
    "                temperature=temperature,\n",
    "                top_p=top_p,\n",
    "                do_sample=True,\n",
    "                length_penalty=0.8\n",
    "            )\n",
    "            response_outputs = outputs[:, len(inputs['input_ids'][0]):]\n",
    "            response = self.tokenizer.batch_decode(response_outputs, skip_special_tokens=True)[0]\n",
    "            return response\n",
    "\n",
    "    def create_chat(self) -> Chat:\n",
    "        return Chat(self)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7860\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize the chatbot\n",
    "chatBot = ChatBot(model_path)\n",
    "conversation = chatBot.create_chat()\n",
    "\n",
    "\n",
    "# Define functions for Gradio interface\n",
    "def chatbot_response(message):\n",
    "    return conversation.send(message)\n",
    "\n",
    "def set_topic(topic):\n",
    "    conversation.set_topic(topic)\n",
    "    return \"Topic set.\"\n",
    "\n",
    "def reset_conversation():\n",
    "    conversation.reset_conversation()\n",
    "    return \"Conversation reset.\"\n",
    "\n",
    "def set_mood(temperature, top_p):\n",
    "    conversation.set_mood(temperature, top_p)\n",
    "    return f\"Mood set to Temperature: {temperature}, Top P: {top_p}\"\n",
    "\n",
    "\n",
    "# Create Gradio interface using Blocks\n",
    "with gr.Blocks() as demo:\n",
    "    gr.Markdown(\"## Customizable Chatbot\")\n",
    "    \n",
    "    # Main Chat Interface\n",
    "    with gr.Row():\n",
    "        chatbot_input = gr.Textbox(label=\"Type your message\", placeholder=\"Send a message to the chatbot\")\n",
    "        chatbot_output = gr.Textbox(label=\"Chatbot's Response\", interactive=False)\n",
    "    \n",
    "    # Chatbot interaction button\n",
    "    with gr.Row():\n",
    "        send_button = gr.Button(\"Send\")\n",
    "    \n",
    "    send_button.click(chatbot_response, inputs=chatbot_input, outputs=chatbot_output)\n",
    "\n",
    "    # Controls for setting mood, topic, and resetting conversation\n",
    "    with gr.Row():\n",
    "        with gr.Column():\n",
    "            topic_input = gr.Textbox(label=\"Set Topic\", placeholder=\"Enter conversation topic\")\n",
    "            topic_button = gr.Button(\"Set Topic\")\n",
    "            topic_button.click(set_topic, inputs=topic_input, outputs=chatbot_output)\n",
    "        \n",
    "        with gr.Column():\n",
    "            temperature_slider = gr.Slider(minimum=0.1, maximum=1.0, label=\"Temperature\", step=0.1, value=0.6)\n",
    "            top_p_slider = gr.Slider(minimum=0.1, maximum=1.0, label=\"Top P\", step=0.1, value=0.9)\n",
    "            mood_button = gr.Button(\"Set Mood\")\n",
    "            mood_button.click(set_mood, inputs=[temperature_slider, top_p_slider], outputs=chatbot_output)\n",
    "        \n",
    "        with gr.Column():\n",
    "            reset_button = gr.Button(\"Reset Conversation\")\n",
    "            reset_button.click(reset_conversation, outputs=chatbot_output)\n",
    "\n",
    "# Launch the Gradio app\n",
    "demo.launch()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
